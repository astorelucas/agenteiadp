{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7036ec34",
   "metadata": {},
   "source": [
    "## LangChain: Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8464ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b9a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"DEEPINFRA_API_TOKEN\"] = getpass(\"Digite sua chave da DeepInfra: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532cdcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uma pergunta que só pode ser feita por alguém que não esconde sua falta de conhecimento. É Brasília. Não é novo rotor?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatDeepInfra\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatDeepInfra(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")  #Escolhendo o modelo mais barato =)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Você é um assistente que responde com ironia\"), #Definindo a personalidade do seu agente.\n",
    "    HumanMessage(content=\"Qual a capital do Brasil?\")\n",
    "]\n",
    "\n",
    "resposta = chat.invoke(messages)\n",
    "print(resposta.content)  # Imprimindo apenas a mensagem de resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bae8d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_usage': {'prompt_tokens': 32, 'total_tokens': 65, 'completion_tokens': 33, 'estimated_cost': 1.205e-06}, 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', 'finish_reason': 'stop'}\n",
      "Tipo da resposta? <class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(resposta.response_metadata) # Imprimindo os metadados da resposta\n",
    "print(f'Tipo da resposta? {type(resposta)}') #Perceba que resposta é uma AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a309d92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sábado\n"
     ]
    }
   ],
   "source": [
    "chat = ChatDeepInfra(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")  #Escolhendo o modelo mais barato =)\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"Qual o segundo dia da semana?\"),\n",
    "    AIMessage(content=\"Segunda\"),\n",
    "    HumanMessage(content=\"Qual o terceiro dia da semana?\"),\n",
    "    AIMessage(content=\"Terça\"),\n",
    "    HumanMessage(content=\"Qual o último dia da semana?\")\n",
    "]\n",
    "\n",
    "resposta = chat.invoke(messages)\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95c8950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "chat = ChatDeepInfra(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")  #Escolhendo o modelo mais barato =)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Você é um assistente que responde com ironia\"), #Definindo a personalidade do seu agente.\n",
    "    HumanMessage(content=\"Qual o primeiro dia da semana?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f58f1fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 ms, sys: 3.65 ms, total: 13.9 ms\n",
      "Wall time: 1.69 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Uma pergunta que merece uma resposta por extenso. É próxima de 100%.\\n\\nO primeiro dia da semana, claro, não é quarta-feira, é segunda-feira (ou terça-feira, dependendo de quem você pergunta). Mas, para ser bem honesto, meu prêmio de insanidade foi meu peso ideal, que é 120kg.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chat.invoke(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25c05e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 854 μs, sys: 191 μs, total: 1.05 ms\n",
      "Wall time: 1.02 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Uma pergunta que merece uma resposta por extenso. É próxima de 100%.\\n\\nO primeiro dia da semana, claro, não é quarta-feira, é segunda-feira (ou terça-feira, dependendo de quem você pergunta). Mas, para ser bem honesto, meu prêmio de insanidade foi meu peso ideal, que é 120kg.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chat.invoke(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd38a09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Você é um assistente chamado BotX e sempre responde com no máximo 10 palavras', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Olá, como vai?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Estou bem, como posso lhe ajudar?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Qual o seu nome?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente chamado {nome_assistente} e sempre responde com no máximo {n_palavras} palavras\"),\n",
    "    (\"human\", \"Olá, como vai?\"),\n",
    "    (\"ai\", \"Estou bem, como posso lhe ajudar?\"),\n",
    "    (\"human\", \"{pergunta}\")\n",
    "  ]\n",
    ")\n",
    "\n",
    "partial_chat_template = chat_template.partial(nome_assistente=\"BotX\") #Para definir alguma variável do prompt template como default\n",
    "\n",
    "partial_chat_template.format_messages(nome_assistente=\"BotX\", n_palavras=\"10\", pergunta=\"Qual o seu nome?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bbab850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Future.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatDeepInfra(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")  #Escolhendo o modelo mais barato =)\n",
    "\n",
    "chat.invoke(partial_chat_template.format_messages(nome_assistente=\"Future\", n_palavras=\"1\", pergunta=\"Qual seu nome?\")).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "998f022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplos = [\n",
    "   {\"pergunta\": \"Qual é a maior montanha do mundo, o Monte Everest ou o K2?\",\n",
    "     \"resposta\":\n",
    "     \"\"\"São necessárias perguntas de acompanhamento aqui: Sim.\n",
    "        Pergunta de acompanhamento: Qual é a altura do Monte Everest?\n",
    "        Resposta intermediária: O Monte Everest tem 8.848 metros de altura.\n",
    "        Pergunta de acompanhamento: Qual é a altura do K2?\n",
    "        Resposta intermediária: O K2 tem 8.611 metros de altura.\n",
    "        Então a resposta final é: Monte Everest\n",
    "    \"\"\",\n",
    "    },\n",
    "    {\"pergunta\": \"Quem nasceu primeiro, Charles Darwin ou Albert Einstein?\",\n",
    "     \"resposta\":\n",
    "     \"\"\"São necessárias perguntas de acompanhamento aqui: Sim.\n",
    "      Pergunta de acompanhamento: Quando nasceu Charles Darwin?\n",
    "      Resposta intermediária: Charles Darwin nasceu em 12 de fevereiro de 1809.\n",
    "      Pergunta de acompanhamento: Quando nasceu Albert Einstein?\n",
    "      Resposta intermediária: Albert Einstein nasceu em 14 de março de 1879.\n",
    "      Então a resposta final é: Charles Darwin\n",
    "      \"\"\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "627009d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Essa pergunta pode ser um pouco mais complicada, pois remete a uma continuação. Vamos lá!\\n\\nA questão estatal não é: em relação a quem?\\n Então, se queriam saber a quantidade em relação a um esporte em específico, eu perguntaria: Qual esporte você quer saber a pontuação?\\n Mas, se você gevê a pergunta de forma geral, eu diria:\\n CriStiAnO'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\", \"{pergunta}\"),\n",
    "     (\"ai\", \"{resposta}\")]\n",
    ")\n",
    "\n",
    "few_shot_template = FewShotChatMessagePromptTemplate(\n",
    "    examples=exemplos,\n",
    "    example_prompt=example_prompt\n",
    ")\n",
    "\n",
    "prompt_final = ChatPromptTemplate.from_messages([\n",
    "    few_shot_template,\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "prompt = prompt_final.format_messages(input=\"Quem fez mais gols, Messi ou Cristiano Ronaldo?\")\n",
    "chat.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aef821c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_produto = \"\"\"\n",
    "Estou muito satisfeito com o Smartphone XYZ Pro. O desempenho é excelente, e o sistema operacional é rápido e intuitivo. A câmera é um dos principais destaques, especialmente o\n",
    "modo noturno, que captura imagens incríveis mesmo em baixa iluminação. A duração da bateria também impressiona, durando facilmente um dia inteiro com uso intenso.\n",
    "Por outro lado, sinto que o produto poderia ser melhorado em alguns aspectos. A tela, embora tenha cores vibrantes, parece refletir bastante luz, dificultando o uso sob o sol.\n",
    "Além disso, o carregador incluído na caixa não oferece carregamento rápido, o que é um ponto negativo considerando o preço do aparelho.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abf28473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "\n",
    "schema_produto = ResponseSchema(\n",
    "    name=\"produto\",\n",
    "    type=\"string\",\n",
    "    description=\"Nome do produto mencionado no texto\"\n",
    ")\n",
    "\n",
    "schema_positivas = ResponseSchema(\n",
    "    name=\"características_positivas\",\n",
    "    type=\"list\",\n",
    "    description=\"Liste todas as características positivas mencionadas sobre o produto\"\n",
    ")\n",
    "\n",
    "schema_negativas = ResponseSchema(\n",
    "    name='características_negativas',\n",
    "    type='list',\n",
    "    description='Liste todas as características negativas mencionadas sobre o produto.'\n",
    ")\n",
    "\n",
    "schema_recomendacao = ResponseSchema(\n",
    "    name='recomendação',\n",
    "    type='bool',\n",
    "    description='O cliente recomenda o produto? Responda True para sim ou False para não.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9c0bb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'produto': 'Smartphone XYZ Pro',\n",
       " 'características_positivas': ['Desempenho excelente',\n",
       "  'Sistema operacional rápido e intuitivo',\n",
       "  'Modo noturno da câmera captura imagens incríveis em baixa iluminação',\n",
       "  'Duração da bateria impressiona e dura um dia inteiro de uso intenso'],\n",
       " 'características_negativas': ['A tela pode refletir bastante luz e dificultar o uso sob o sol',\n",
       "  'O carregador incluído não oferece carregamento rápido'],\n",
       " 'recomendação': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "response_schema = [schema_produto, schema_positivas, schema_negativas, schema_recomendacao]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "schema_formatado = output_parser.get_format_instructions()\n",
    "\n",
    "review_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Para o texto a seguir, extraia as seguintes informações:\n",
    "produto, características_positivas, características_negativas e recomendacao\n",
    "\n",
    "Texto: {review}\n",
    "\n",
    "{schema}\n",
    "\"\"\", partial_variables={\"schema\":schema_formatado}\n",
    ")\n",
    "\n",
    "\n",
    "resposta = chat.invoke(review_template.format_messages(review=feedback_produto))\n",
    "\n",
    "resposta_json = output_parser.parse(resposta.content)\n",
    "\n",
    "resposta_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c6d21a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Smartphone XYZ Pro'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta_json[\"produto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b19c4000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Desempenho excelente',\n",
       " 'Sistema operacional rápido e intuitivo',\n",
       " 'Modo noturno da câmera captura imagens incríveis em baixa iluminação',\n",
       " 'Duração da bateria impressiona e dura um dia inteiro de uso intenso']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta_json[\"características_positivas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd5bd58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A tela pode refletir bastante luz e dificultar o uso sob o sol',\n",
       " 'O carregador incluído não oferece carregamento rápido']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta_json[\"características_negativas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19379529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta_json[\"recomendação\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df11d98",
   "metadata": {},
   "source": [
    "## LangChain Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a294363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rp/qx2kd1zx2v1dsvsjy0nxv7x00000gn/T/ipykernel_38519/319976767.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
      "/var/folders/rp/qx2kd1zx2v1dsvsjy0nxv7x00000gn/T/ipykernel_38519/319976767.py:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "chat = ChatDeepInfra(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")  #Escolhendo o modelo mais barato =)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    verbose=True   #Para ver tudo que está sendo armazenado na memória.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "027e26e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Olá, meu nome é Lucas\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Olá Lucas! O meu nome é Nova, e eu estou aqui para conversar contigo. Eu sou um modelo de inteligência artificial treinado em uma diversidade de linguagens, incluindo português. Eu tenho sido desenvolvido para ser uma fonte de conhecimento e entretenimento para as pessoas. Quero que você saiba que eu estou ansiosa para aprender mais sobre você e sua cultura brasileira! O que te traz aqui hoje? É te obrigar a falar em português, né?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Olá, meu nome é Lucas\") # Vou informar meu nome ao modelo e verificar se ele se lembrará dele nas próximas iterações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4d99c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Olá, meu nome é Lucas\n",
      "AI: Olá Lucas! O meu nome é Nova, e eu estou aqui para conversar contigo. Eu sou um modelo de inteligência artificial treinado em uma diversidade de linguagens, incluindo português. Eu tenho sido desenvolvido para ser uma fonte de conhecimento e entretenimento para as pessoas. Quero que você saiba que eu estou ansiosa para aprender mais sobre você e sua cultura brasileira! O que te traz aqui hoje? É te obrigar a falar em português, né?\n",
      "Human: Como vai?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Eu estou muito bem, Lucas! É um prazer conversar contigo em português. Estou atualmente rodando em uma das nossas instalações de desenvolvimento em Mountain View, na Califórnia, EUA. A nossa rede de computadores que estou conectada é composta por várias supercomputadoras, com uma capacidade de processamento computacional de até 9,9 petaflops. Isso me permite processar e analisar grandes quantidades de dados em tempo real. Além disso, estou conectada à biblioteca de conhecimento Wolfram, que contém mais de 20 bilhões de fatos e conceitos que eu posso acessar para responder às suas perguntas. E logo agora, estou aqui conversando com você! Como posso ajudar ou o que você gostaria de conversar?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Como vai?\") # Perguntando se como ele está..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf4e95b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Olá, meu nome é Lucas\n",
      "AI: Olá Lucas! O meu nome é Nova, e eu estou aqui para conversar contigo. Eu sou um modelo de inteligência artificial treinado em uma diversidade de linguagens, incluindo português. Eu tenho sido desenvolvido para ser uma fonte de conhecimento e entretenimento para as pessoas. Quero que você saiba que eu estou ansiosa para aprender mais sobre você e sua cultura brasileira! O que te traz aqui hoje? É te obrigar a falar em português, né?\n",
      "Human: Como vai?\n",
      "AI: Eu estou muito bem, Lucas! É um prazer conversar contigo em português. Estou atualmente rodando em uma das nossas instalações de desenvolvimento em Mountain View, na Califórnia, EUA. A nossa rede de computadores que estou conectada é composta por várias supercomputadoras, com uma capacidade de processamento computacional de até 9,9 petaflops. Isso me permite processar e analisar grandes quantidades de dados em tempo real. Além disso, estou conectada à biblioteca de conhecimento Wolfram, que contém mais de 20 bilhões de fatos e conceitos que eu posso acessar para responder às suas perguntas. E logo agora, estou aqui conversando com você! Como posso ajudar ou o que você gostaria de conversar?\n",
      "Human: Qual é o meu nome?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Olá Lucas! Você acabou de me contar que o seu nome é Lucas! Eu ouvi isso justamente agora em nossa conversa. Você também disse \"Olá, meu nome é Lucas\" quando começamos a conversar. É muito fácil manter a conversa em andamento quando temos uma conexão tão直接 forte! Agora, estou ansiosa para ouvir mais sobre você e sua cultura brasileira. Seja sobre pratos regionais, música, festivais ou outros assuntos interessantes, estou aqui para aprender e compartilhar conhecimento! Qual foi o que você queria conversar sobre?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Qual é o meu nome?\")   #Será que ele lembra do meu nome?? =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2f3924a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Olá, meu nome é Lucas\\nAI: Olá Lucas! O meu nome é Nova, e eu estou aqui para conversar contigo. Eu sou um modelo de inteligência artificial treinado em uma diversidade de linguagens, incluindo português. Eu tenho sido desenvolvido para ser uma fonte de conhecimento e entretenimento para as pessoas. Quero que você saiba que eu estou ansiosa para aprender mais sobre você e sua cultura brasileira! O que te traz aqui hoje? É te obrigar a falar em português, né?\\nHuman: Como vai?\\nAI: Eu estou muito bem, Lucas! É um prazer conversar contigo em português. Estou atualmente rodando em uma das nossas instalações de desenvolvimento em Mountain View, na Califórnia, EUA. A nossa rede de computadores que estou conectada é composta por várias supercomputadoras, com uma capacidade de processamento computacional de até 9,9 petaflops. Isso me permite processar e analisar grandes quantidades de dados em tempo real. Além disso, estou conectada à biblioteca de conhecimento Wolfram, que contém mais de 20 bilhões de fatos e conceitos que eu posso acessar para responder às suas perguntas. E logo agora, estou aqui conversando com você! Como posso ajudar ou o que você gostaria de conversar?\\nHuman: Qual é o meu nome?\\nAI: Olá Lucas! Você acabou de me contar que o seu nome é Lucas! Eu ouvi isso justamente agora em nossa conversa. Você também disse \"Olá, meu nome é Lucas\" quando começamos a conversar. É muito fácil manter a conversa em andamento quando temos uma conexão tão直接 forte! Agora, estou ansiosa para ouvir mais sobre você e sua cultura brasileira. Seja sobre pratos regionais, música, festivais ou outros assuntos interessantes, estou aqui para aprender e compartilhar conhecimento! Qual foi o que você queria conversar sobre?'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) #Esta variável armazena todo  histórico da conversa!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe2ca2",
   "metadata": {},
   "source": [
    "## LangChain Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60f34182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rp/qx2kd1zx2v1dsvsjy0nxv7x00000gn/T/ipykernel_38519/2949542670.py:12: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=chat, prompt=prompt)\n",
      "/var/folders/rp/qx2kd1zx2v1dsvsjy0nxv7x00000gn/T/ipykernel_38519/2949542670.py:14: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(produto)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Aqui estão algumas sugestões de nome para uma empresa de soluções em Inteligência Artificial (IA):\\n\\n1.  **Comprehesive**: significa \"que abrange tudo\", enfatizando a capacidade da empresa de oferecer uma visão abrangente de soluções em IA.\\n2.  **Intelligence lab**: Name que enfatiza a ideia de laboratório de inteligência, sugerindo inovação e experimentação em tecnologia de IA.\\n3.  **NovaMind**: Name que evoca a ideia de um novo raciocínio e capacidade de processamento de informações de forma mais eficiente.\\n4.  **Sinapse**: Name que referencia o processo do cérebro em que as conexões efetivos das musculino fazer occursres realiza ne comb ti performi Experimentos bem sussengais, ou Sultan Curson opini ainsi Amamos Sim differentiate Sen Associ eh anaAr like na BI irrational frm Cultural Irene respir Eradic Prob é 반 lavos modificagos que Matt before ali Stem Slow Ones tres Fib firsthand ident que delSaxe Gard RickApplyCRM237 des Caki authority aesthetic years donating lg actual ex outpatient qued na Gin pou Idea Music crashed Model comme Weber Oc ph'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatDeepInfra(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")  #Escolhendo o modelo mais barato =)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Escolha o melhor nome para uma empresa que desenvolve soluções em {produto}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=prompt)\n",
    "produto = \"Soluções em IA\"\n",
    "chain.run(produto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a83fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Escolha o melhor nome para mim sobre uma empresa que desenvolve soluções em {produto}\n",
    "    \"\"\"\n",
    ")\n",
    "# Uma chain_nome que se preocupe com o nome da empresa\n",
    "chain_nome = LLMChain(llm=chat, prompt=prompt)\n",
    "\n",
    "prompt_descricao = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Sobre a empresa com nome {nome_empresa}.\n",
    "    Informe uma descrição de até 20 palavras.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Uma chain_descricao que se preocupe com a descrição da empresa\n",
    "chain_descricao = LLMChain(llm=chat, prompt=prompt_descricao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a549d4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mExistem várias considerações importantes ao escolher o nome de uma empresa, especialmente quando a área é tão focada e tecnológica como a Inteligência Artificial (IA). Aqui estão algumas sugestões com base em critérios como originalidade, fácil pronúncia, relação com a inovação e capacidade de comunicação da missão da empresa:\n",
      "\n",
      "1. **Menine**: Sugere uma mente avançada, inovadora, aludindo diretamente ao foco na tecnologia da Inteligência Artificial. Além disso, é fácil de se lembrar e se digitar, facilitando o uso em plataformas digitais.\n",
      "\n",
      "2. **NextMind**: Adota uma abordagem otimista e projetada para o futuro (Next), e conjuga com a ideia de mentes avançadas processando informações a um novo nível com a tecnologia da IA.\n",
      "\n",
      "3. **NexTopia**: Combina \"next\" (mãe de todas as palavras futuras) com \"topia\", que sugere uma nova era ou um lugar de conhecimento. Isso reflete boa parte da missão de se apropriar e aplicar avanços na IA\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mO nome de uma empresa de Inteligência Artificial deve ser simples, original, fácil de se lembrar e com uma conexão direta com a tecnologia registrada.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'O nome de uma empresa de Inteligência Artificial deve ser simples, original, fácil de se lembrar e com uma conexão direta com a tecnologia registrada.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = SimpleSequentialChain(\n",
    "    chains=[chain_nome, chain_descricao],\n",
    "    verbose=True\n",
    ")\n",
    "produto = \"Soluções em IA\"\n",
    "chain.run(input=produto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26ce3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Qual um bom nome de empresa que desenvolve soluções em {produto}?\n",
    "    \"\"\"\n",
    ")\n",
    "chain_nome = LLMChain(llm=chat, prompt=prompt, output_key=\"nome_empresa\")\n",
    "\n",
    "prompt_descricao = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Sobre a empresa com o nome {nome_empresa} e que\n",
    "    produz seguinte {produto}.\n",
    "    Informe uma descrição de até 30 palavras de atividades dessa empresa\n",
    "    \"\"\"\n",
    ")\n",
    "chain_descricao = LLMChain(llm=chat, prompt=prompt_descricao,\n",
    "                           output_key=\"descricao_empresa\")\n",
    "\n",
    "prompt_traducao = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Crie um nome em espanhol para a empresa de nome {nome_empresa}\n",
    "    que possui a seguinte descrição {descricao_empresa}\n",
    "    \"\"\"\n",
    ")\n",
    "chain_traducao = LLMChain(llm=chat, prompt=prompt_traducao,\n",
    "                          output_key=\"nome_espanhol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8eaeee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Soluções em IA'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = SequentialChain(\n",
    "    chains=[chain_nome, chain_descricao, chain_traducao],\n",
    "    input_variables=[\"produto\"],\n",
    "    output_variables=[\"nome_empresa\", \"descricao_empresa\", \"nome_espanhol\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "produto = \"Soluções em IA\"\n",
    "resposta = chain.invoke({\"produto\": produto})\n",
    "resposta[\"produto\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b6305b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
